# Adversial-Attacks-on-LLM-Code-Synthesis

# Attacking an LLM code synthesis chain with access to the model
## Injecting bad code onto code synthesis output

# Prompt Injection
All LLM output should be considered potentially dangerous.
## Langchain Prompt Injection Examples
https://security.snyk.io/package/pip/langchain

# Supply chain attacks
Compromising a popular LLM model or library to inject malicious code.
